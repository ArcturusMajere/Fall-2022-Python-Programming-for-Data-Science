{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPG5INqg-_qn"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/avakanski/Fall-2022-Python-Programming-for-Data-Science/blob/main/Lectures/Theme%203%20-%20Model%20Engineering%20Pipelines/Lecture%2018%20-%20Natural%20Language%20Processing/Lecture%2018%20-%20Natural%20Language%20Processing.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JhHIQqNe4Qs"
      },
      "source": [
        "<a name='section0'></a>\n",
        "# Lecture 19 Transformer Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEkmemKte4Qv"
      },
      "source": [
        "- [19.1 Introduction to Tranformers)](#section1)\n",
        "- [19.2 Self-attention Mechanism](#section2)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTFgm_bJe4Qw"
      },
      "source": [
        "<a name='section1'></a>\n",
        "\n",
        "# 19.1 Introduction to Transformers "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFMFGCdwiw5k"
      },
      "source": [
        "***Transformer Neural Networks***, or simply ***Transformers***, is a neural network architecture introduced in 2017 in the now famous paper [“Attention is all you need”](https://arxiv.org/abs/1706.03762). The title reffers to the attention mechanism, which forms the basis for data processing with Transformers.  \n",
        "\n",
        "Transformer models have been the predominant type of deep learning models used for NLP in recent years. They have outperformed Recurrent Neural Networks in all NLP tasks, and all Large Language Models employ the Transformer network architecture. As well as, Transformers networks were recently adapted for other taks and have outperformed other machine learning models for image processing and video processing tasks, protein and DNA sequence prediction, timeseries data, as well have been used for reinforcement learning tasks. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF5bogcC7mUa"
      },
      "source": [
        "<a name='section2'></a>\n",
        "\n",
        "# 19.2 Self-attention Mechanism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWMSWyTqjNym"
      },
      "source": [
        "***Self-attention*** in neural networks is a mechanism that forces a model to attend to portions of the data when making predictions. For instance, in NLP, self-attention mechanism is used to identify words in sentences that have significance for a given (query) word in the sentence. \n",
        "\n",
        "It is obvious to most that in the first sentence pair “it” refers to the animal, and in the second to the street. When translating these sentences to French or German, the translation for “it” depends on the gender of the noun it refers to - and in French “animal” and “street” have different genders. In contrast to the current Google Translate model, the Transformer translates both of these sentences to French correctly. Visualizing what words the encoder attended to when computing the final representation for the word “it” sheds some light on how the network made the decision. In one of its steps, the Transformer clearly identified the two nouns “it” could refer to and the respective amount of attention reflects its choice in the different contexts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "work in progress ..."
      ],
      "metadata": {
        "id": "CTj6ItI1dOiu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vweobvFVe4RB"
      },
      "source": [
        "<a name='section6'></a>\n",
        "\n",
        "# References\n",
        "\n",
        "1. Complete Machine Learning Package, Jean de Dieu Nyandwi, available at: [https://github.com/Nyandwi/machine_learning_complete](https://github.com/Nyandwi/machine_learning_complete).\n",
        "2. Deep Learning with Python, Francois Chollet, Second Edition, Manning Publications, 2021.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDbzcXmWe4RB"
      },
      "source": [
        "[BACK TO TOP](#section0)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}